{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "545531d2-0a13-4494-b646-22ef1863191b",
   "metadata": {},
   "source": [
    "# Clinical Quesition Answering Using BERT\n",
    "\n",
    "What if we want to support any question a physician might want to ask instead of simpler rule-based questions such as \"is a disease present\"? To do this, we'll have to use more recent artificial intelligence techniques and large datasets as opposed to mere classification based on tabular data. In this project we go through the pre- and post-processing involved in applying [BERT](https://github.com/google-research/bert) to the problem of question answering. After developing this infrastructure, we use the model to answer questions from clinical notes.\n",
    "\n",
    "Implementing question answering can take a few steps, even using pretrained models. \n",
    "- First retrieve our model and tokenizer (preparing the input), mapping each word to a unique element in the vocabulary and inserting special tokens. \n",
    "- Then, the model processes these tokenized inputs to create valuable embeddings and performs tasks such as question answering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07b0af1e-f1bc-489e-960b-2178e56e02fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cf1198-fef0-48c6-83a0-22b9d1d9645e",
   "metadata": {},
   "source": [
    "## 1 - Load the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb72091b-14a4-41b2-afb2-1b47fc84021c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./models/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./models\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"eos_token_ids\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.39.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file ./models/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./models\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"eos_token_ids\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.39.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file ./models/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./models\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"eos_token_ids\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.39.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5b2950",
   "metadata": {},
   "source": [
    "## 2 - Preparing the Input\n",
    "\n",
    "Our first task will be ttohe prepare the raw passage and question for input into the model. \n",
    "\n",
    "Given the strings `p` and `q`, we want to turn them into an input of the following form: \n",
    "\n",
    "`[CLS]` `[q_token1]`, `[q_token2]`, ..., `[SEP]` `[p_token1]`, `[p_token2]`, ...\n",
    "\n",
    "Here, the special characters `[CLS]` and `[SEP]` let the model know which part of the input is the question and which is the answer. \n",
    "- The question appears between `[CLS]` and `[SEP]`.\n",
    "- The answer appears after `[SEP]`\n",
    "\n",
    "We'll also pad the input to the max input length, since BERT takes in a fixed-length input.\n",
    "\n",
    "We'll return three items:\n",
    "- First is `input_ids`, which holds the numerical ids of each token. \n",
    "- Second, we'll output the `input_mask`, which has 1's in parts of the input tensor representing input tokens, and 0's where there is padding. \n",
    "- Finally, we'll output `tokens`, the output of the tokenizer (including the `[CLS]` and `[SEP]` tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40931dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_bert_input(question, passage, tokenizer, max_seq_length=384):\n",
    "    \"\"\"\n",
    "    Prepare question and passage for input to BERT. \n",
    "\n",
    "    Args:\n",
    "        question (string): question string\n",
    "        passage (string): passage string where answer should lie\n",
    "        tokenizer (Tokenizer): used for transforming raw string input\n",
    "        max_seq_length (int): length of BERT input\n",
    "    \n",
    "    Returns:\n",
    "        input_ids (tf.Tensor): tensor of size (1, max_seq_length) which holds\n",
    "                               ids of tokens in input\n",
    "        input_mask (list): list of length max_seq_length of 1s and 0s with 1s\n",
    "                           in indices corresponding to input tokens, 0s in\n",
    "                           indices corresponding to padding\n",
    "        tokens (list): list of length of actual string tokens corresponding to input_ids\n",
    "    \"\"\"\n",
    "    # tokenize question\n",
    "    question_tokens = tokenizer.tokenize(question)\n",
    "    \n",
    "    # tokenize passage\n",
    "    passage_token = tokenizer.tokenize(passage)\n",
    "\n",
    "    # get special tokens \n",
    "    CLS = tokenizer.cls_token\n",
    "    SEP = tokenizer.sep_token\n",
    "        \n",
    "    # manipulate tokens to get input in correct form (not adding padding yet)\n",
    "    # CLS {question_tokens} SEP {answer_tokens} \n",
    "    tokens = [CLS] + question_tokens + [SEP] + passage_token\n",
    "\n",
    "    # Convert tokens into integer IDs\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    \n",
    "    # Create an input mask which has integer 1 for each token in the 'tokens' list\n",
    "    input_mask = [1] * len(tokens)\n",
    "\n",
    "    # pad input_ids with 0s until it is the max_seq_length\n",
    "    input_ids += [0] * (max_seq_length - len(input_ids))\n",
    "    \n",
    "    # Do the same to pad the input_mask so its length is max_seq_length\n",
    "    input_mask += [0] * (max_seq_length - len(input_mask))\n",
    "\n",
    "    return tf.expand_dims(tf.convert_to_tensor(input_ids), 0), input_mask, tokens  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a439f01",
   "metadata": {},
   "source": [
    "## 3 - Getting Answer from Model Output\n",
    "\n",
    "After taking in the tokenized input, the model outputs two vectors. \n",
    "- The first vector contains the scores (more formally, logits) for the starting index of the answer. \n",
    "    - A higher score means that index is more likely to be the start of the answer span in the passage. \n",
    "- The second vector contains the score for the end index of the answer. \n",
    "\n",
    "We want to output the span that maximizes the start score and end score. \n",
    "- To be valid, the start index has to occur before the end index. Formally, yweou want to find:\n",
    "\n",
    "$$\\arg\\max_{i <= j, mask_i=1, mask_j = 1} start\\_scores[i] + end\\_scores[j]$$\n",
    "- In words, this formulas is saying, calculate the sum and start scores of start position 'i' and end position 'j', given the constraint that the start 'i' is either before or at the end position 'j'; then find the positions 'i' and 'j' where this sum is the highest.\n",
    "- Furthermore, we want to make sure that $i$ and $j$ are in the relevant parts of the input (i.e. where `input_mask` equals 1.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba3a49e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span_from_scores(start_scores, end_scores, input_mask, verbose=False):\n",
    "    \"\"\"\n",
    "    Find start and end indices that maximize sum of start score\n",
    "    and end score, subject to the constraint that start is before end\n",
    "    and both are valid according to input_mask.\n",
    "\n",
    "    Args:\n",
    "        start_scores (list): contains scores for start positions, shape (1, n)\n",
    "        end_scores (list): constains scores for end positions, shape (1, n)\n",
    "        input_mask (list): 1 for valid positions and 0 otherwise\n",
    "    \"\"\"\n",
    "    n = len(start_scores)\n",
    "    max_start_i = -1\n",
    "    max_end_j = -1\n",
    "    max_start_val = -np.inf\n",
    "    max_end_val = -np.inf\n",
    "    max_sum = -np.inf\n",
    "    \n",
    "    # Find i and j that maximizes start_scores[i] + end_scores[j]\n",
    "    # so that i <= j and input_mask[i] == input_mask[j] == 1\n",
    "\n",
    "    # Ensure start_scores and end_scores are in a numeric format (e.g., list of floats)\n",
    "    try:\n",
    "        start_scores = [float(score) for score in start_scores]\n",
    "    except ValueError:\n",
    "        raise ValueError(\"start_scores contains non-numeric values\")\n",
    "\n",
    "    try:\n",
    "        end_scores = [float(score) for score in end_scores]\n",
    "    except ValueError:\n",
    "        raise ValueError(\"end_scores contains non-numeric values\")\n",
    "    \n",
    "    # set the range for i\n",
    "    for i in range(n):\n",
    "        \n",
    "        # set the range for j\n",
    "        for j in range(i, n):\n",
    "\n",
    "            # both input masks should be 1\n",
    "            if input_mask[i] == input_mask[j] == 1:\n",
    "                \n",
    "                # check if the sum of the start and end scores is greater than the previous max sum\n",
    "                if (start_scores[i] + end_scores[j]) > max_sum:\n",
    "\n",
    "                    # calculate the new max sum\n",
    "                    max_sum = start_scores[i] + end_scores[j]\n",
    "        \n",
    "                    # save the index of the max start score\n",
    "                    max_start_i = i\n",
    "                \n",
    "                    # save the index for the max end score\n",
    "                    max_end_j = j\n",
    "                    \n",
    "                    # save the value of the max start score\n",
    "                    max_start_val = start_scores[i]\n",
    "                    \n",
    "                    # save the value of the max end score\n",
    "                    max_end_val = end_scores[j]\n",
    "                                        \n",
    "    if verbose:\n",
    "        print(f\"max start is at index i={max_start_i} and score {max_start_val}\")\n",
    "        print(f\"max end is at index i={max_end_j} and score {max_end_val}\")\n",
    "        print(f\"max start + max end sum of scores is {max_sum}\")\n",
    "    return max_start_i, max_end_j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b0c8d7",
   "metadata": {},
   "source": [
    "### 3.1 - Construct the Answer\n",
    "\n",
    "We need to add some post-processing to get the final string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2127729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_answer(tokens):\n",
    "    \"\"\"\n",
    "    Combine tokens into a string, remove some hash symbols, and leading/trailing whitespace.\n",
    "    Args:\n",
    "        tokens: a list of tokens (strings)\n",
    "    \n",
    "    Returns:\n",
    "        out_string: the processed string.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # join the tokens together with whitespace\n",
    "    out_string = ' '.join(tokens)\n",
    "    \n",
    "    # replace ' ##' with empty string\n",
    "    out_string = out_string.replace(' ##', '')\n",
    "    \n",
    "    # remove leading and trailing whitespace\n",
    "    out_string = out_string.strip()\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # if there is an '@' symbol in the tokens, remove all whitespace\n",
    "    if '@' in tokens:\n",
    "        out_string = out_string.replace(' ', '')\n",
    "\n",
    "    return out_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7f787f",
   "metadata": {},
   "source": [
    "## 4 - Putting It All Together\n",
    "\n",
    "The `get_model_answer` function takes all the functions that we've implemented and performs question-answering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be5d62eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./models/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./models\",\n",
      "  \"architectures\": [\n",
      "    \"BertForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"eos_token_ids\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.39.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file ./models/tf_model.h5\n",
      "All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "All the layers of TFBertForQuestionAnswering were initialized from the model checkpoint at ./models.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForQuestionAnswering.from_pretrained(\"./models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c977195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_answer(model, question, passage, tokenizer, max_seq_length=384):\n",
    "    \"\"\"\n",
    "    Identify answer in passage for a given question using BERT. \n",
    "\n",
    "    Args:\n",
    "        model (Model): pretrained Bert model which we'll use to answer questions\n",
    "        question (string): question string\n",
    "        passage (string): passage string\n",
    "        tokenizer (Tokenizer): used for preprocessing of input\n",
    "        max_seq_length (int): length of input for model\n",
    "        \n",
    "    Returns:\n",
    "        answer (string): answer to input question according to model\n",
    "    \"\"\" \n",
    "    # prepare input: use the function prepare_bert_input\n",
    "    input_ids, input_mask, tokens = prepare_bert_input(question, passage, tokenizer, max_seq_length)\n",
    "    \n",
    "    # get scores for start of answer and end of answer\n",
    "    # use the model returned by TFAutoModelForQuestionAnswering.from_pretrained(\"./models\")\n",
    "    # pass in in the input ids that are returned by prepare_bert_input\n",
    "    start_scores, end_scores = model(input_ids)\n",
    "    \n",
    "    # start_scores and end_scores will be tensors of shape [1,max_seq_length]\n",
    "    # To pass these into get_span_from_scores function, \n",
    "    # take the value at index 0 to get a tensor of shape [max_seq_length]\n",
    "    start_scores = start_scores[0]\n",
    "    end_scores = end_scores[0]\n",
    "    \n",
    "    # using scores, get most likely answer\n",
    "    # use the get_span_from_scores function\n",
    "    span_start, span_end = get_span_from_scores(start_scores, end_scores, input_mask)\n",
    "    \n",
    "    # Using array indexing to get the tokens from the span start to span end (including the span_end)\n",
    "    answer_tokens = tokens[span_start:span_end+1]\n",
    "    \n",
    "    # Combine the tokens into a single string and perform post-processing\n",
    "    # use construct_answer\n",
    "    answer = construct_answer(answer_tokens)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f2bad0",
   "metadata": {},
   "source": [
    "## 5 - Experimentation!\n",
    "\n",
    "Now let's try it on clinical notes. Below we have an excerpt of a doctor's notes for a patient with an abnormal echocardiogram (this sample is taken from [here](https://www.mtsamples.com/site/pages/sample.asp?Type=6-Cardiovascular%20/%20Pulmonary&Sample=1597-Abnormal%20Echocardiogram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb34b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "passage = \"Abnormal echocardiogram findings and followup. Shortness of breath, congestive heart failure, \\\n",
    "           and valvular insufficiency. The patient complains of shortness of breath, which is worsening. \\\n",
    "           The patient underwent an echocardiogram, which shows severe mitral regurgitation and also large \\\n",
    "           pleural effusion. The patient is an 86-year-old female admitted for evaluation of abdominal pain \\\n",
    "           and bloody stools. The patient has colitis and also diverticulitis, undergoing treatment. \\\n",
    "           During the hospitalization, the patient complains of shortness of breath, which is worsening. \\\n",
    "           The patient underwent an echocardiogram, which shows severe mitral regurgitation and also large \\\n",
    "           pleural effusion. This consultation is for further evaluation in this regard. As per the patient, \\\n",
    "           she is an 86-year-old female, has limited activity level. She has been having shortness of breath \\\n",
    "           for many years. She also was told that she has a heart murmur, which was not followed through \\\n",
    "           on a regular basis.\"\n",
    "\n",
    "q1 = \"How old is the patient?\"\n",
    "q2 = \"Does the patient have any complaints?\"\n",
    "q3 = \"What is the reason for this consultation?\"\n",
    "q4 = \"What does her echocardiogram show?\"\n",
    "q5 = \"What other symptoms does the patient have?\"\n",
    "\n",
    "\n",
    "questions = [q1, q2, q3, q4, q5]\n",
    "\n",
    "for i, q in enumerate(questions):\n",
    "    print(\"Question {}: {}\".format(i+1, q))\n",
    "    print()\n",
    "    print(\"Answer: {}\".format(get_model_answer(model, q, passage, tokenizer)))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ebfe31",
   "metadata": {},
   "source": [
    "### Output:\n",
    "\n",
    "Question 1: How old is the patient?\n",
    "\n",
    "Answer: 86\n",
    "\n",
    "\n",
    "Question 2: Does the patient have any complaints?\n",
    "\n",
    "Answer: The patient complains of shortness of breath\n",
    "\n",
    "\n",
    "Question 3: What is the reason for this consultation?\n",
    "\n",
    "Answer: further evaluation\n",
    "\n",
    "\n",
    "Question 4: What does her echocardiogram show?\n",
    "\n",
    "Answer: severe mitral regurgitation and also large pleural effusion\n",
    "\n",
    "\n",
    "Question 5: What other symptoms does the patient have?\n",
    "\n",
    "Answer: colitis and also diverticulitis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18e4f63",
   "metadata": {},
   "source": [
    "Even without fine-tuning, the model is able to reasonably answer most of the questions! Of course, it isn't perfect (it doesn't give much detail). To improve performance, we would ideally collect a medical QA dataset and fine tune the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0468e859",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
